{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Fundamentos de IA ‚Äî Entrenamiento Completo de Modelos\n",
    "\n",
    "**Instructor:** Alexander  \n",
    "**Duraci√≥n:** 4-5 horas  \n",
    "**Nivel:** Intermedio-Avanzado\n",
    "\n",
    "## üìã Objetivos de Aprendizaje\n",
    "\n",
    "‚úÖ Comprender hiperpar√°metros y c√≥mo afectan el aprendizaje  \n",
    "‚úÖ Dominar algoritmos de optimizaci√≥n (SGD, Adam, RMSprop)  \n",
    "‚úÖ Aplicar Feature Engineering para mejorar modelos  \n",
    "‚úÖ Implementar y comparar m√∫ltiples algoritmos  \n",
    "‚úÖ Optimizar modelos de manera sistem√°tica  \n",
    "‚úÖ Desplegar modelos en producci√≥n  \n",
    "\n",
    "## üìö Contenido Expandido\n",
    "\n",
    "1. Configuraci√≥n del entorno\n",
    "2. **NUEVO:** Conceptos fundamentales de ML\n",
    "3. **NUEVO:** Hiperpar√°metros explicados (Learning Rate, Epochs, etc.)\n",
    "4. **NUEVO:** Algoritmos de optimizaci√≥n (SGD, Adam, RMSprop)\n",
    "5. Exploraci√≥n y preparaci√≥n de datos\n",
    "6. **EXPANDIDO:** Feature Engineering y Feature Selection\n",
    "7. Modelos de clasificaci√≥n (14+ algoritmos)\n",
    "8. Modelos de regresi√≥n (11+ algoritmos)\n",
    "9. Evaluaci√≥n y m√©tricas avanzadas\n",
    "10. Pipelines y automatizaci√≥n\n",
    "11. **EXPANDIDO:** Optimizaci√≥n de hiperpar√°metros pr√°ctica\n",
    "12. Manejo de desbalanceo de clases\n",
    "13. Validaci√≥n cruzada avanzada\n",
    "14. Guardado y despliegue\n",
    "15. Ejercicios y proyecto final\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configuraci√≥n del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n (si es necesario)\n",
    "# !pip install scikit-learn pandas numpy matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n visual\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.datasets import load_iris, load_diabetes, load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import joblib\n",
    "import sklearn\n",
    "\n",
    "print('‚úÖ Librer√≠as cargadas')\n",
    "print(f'üì¶ Scikit-learn: {sklearn.__version__}')\n",
    "print(f'üì¶ NumPy: {np.__version__}')\n",
    "print(f'üì¶ Pandas: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Conceptos Fundamentales de Machine Learning\n",
    "\n",
    "### üß† ¬øC√≥mo Aprende un Modelo?\n",
    "\n",
    "Imagina que est√°s aprendiendo a lanzar una pelota a una canasta:\n",
    "\n",
    "1. **Intentas** ‚Üí Observas si acertaste o fallaste\n",
    "2. **Ajustas** tu fuerza y √°ngulo\n",
    "3. **Repites** hasta mejorar\n",
    "\n",
    "Los modelos de ML hacen lo mismo:\n",
    "\n",
    "```\n",
    "BUCLE DE ENTRENAMIENTO:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  1. PREDICCI√ìN (Forward Pass)                          ‚îÇ\n",
    "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
    "‚îÇ     ‚îÇ  Datos   ‚îÇ ‚îÄ‚îÄ‚Üí [Modelo] ‚îÄ‚îÄ‚Üí Predicci√≥n          ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  2. CALCULAR ERROR (Loss Function)                     ‚îÇ\n",
    "‚îÇ     Error = |Predicci√≥n - Valor Real|                  ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  3. ACTUALIZAR PESOS (Backward Pass)                   ‚îÇ\n",
    "‚îÇ     Peso_nuevo = Peso_viejo - (Learning_Rate √ó Error)  ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  4. REPETIR ‚Ü∫ (Epochs)                                 ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üéØ Elementos Clave\n",
    "\n",
    "1. **Pesos (Weights)**: N√∫meros que el modelo **aprende** durante entrenamiento\n",
    "2. **Hiperpar√°metros**: Configuraciones que **t√∫ defines** antes de entrenar\n",
    "3. **Funci√≥n de p√©rdida (Loss)**: Mide qu√© tan mal lo hace el modelo\n",
    "4. **Optimizador**: Algoritmo que actualiza los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Hiperpar√°metros Explicados\n",
    "\n",
    "### üìñ Definici√≥n\n",
    "\n",
    "**Hiperpar√°metros** = Configuraciones que controlan **c√≥mo** aprende el modelo.\n",
    "\n",
    "üî¥ **NO** los aprende el modelo  \n",
    "üü¢ **S√ç** los defines t√∫ antes de entrenar\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Hiperpar√°metros M√°s Importantes\n",
    "\n",
    "### 1Ô∏è‚É£ **Learning Rate (Œ± o Œ∑) - Tasa de Aprendizaje**\n",
    "\n",
    "**¬øQu√© es?**  \n",
    "Controla **qu√© tan grandes son los pasos** que da el modelo al ajustar sus pesos.\n",
    "\n",
    "**Analog√≠a:**  \n",
    "Imagina que bajas una monta√±a con los ojos vendados:\n",
    "- **Learning Rate alto** (Œ± = 1.0): Das pasos gigantes ‚Üí R√°pido pero puedes pasarte del valle\n",
    "- **Learning Rate bajo** (Œ± = 0.001): Das pasos peque√±os ‚Üí Lento pero llegas al fondo\n",
    "\n",
    "```python\n",
    "# F√≥rmula de actualizaci√≥n de pesos:\n",
    "peso_nuevo = peso_viejo - (learning_rate √ó gradiente)\n",
    "```\n",
    "\n",
    "**Valores t√≠picos:**\n",
    "- Para SGD: `0.01` a `0.1`\n",
    "- Para Adam: `0.001` (default)\n",
    "- Para redes neuronales: `0.0001` a `0.01`\n",
    "\n",
    "**Efectos:**\n",
    "- **Muy alto** ‚Üí Modelo oscila, no converge\n",
    "- **Muy bajo** ‚Üí Entrenamiento muy lento, se queda en m√≠nimos locales\n",
    "- **√ìptimo** ‚Üí Converge r√°pido y estable\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Epochs (√âpocas)**\n",
    "\n",
    "**¬øQu√© es?**  \n",
    "Una **epoch** = 1 pasada completa por TODOS los datos de entrenamiento.\n",
    "\n",
    "**Ejemplo:**  \n",
    "Si tienes 1000 im√°genes y entrenas por 10 epochs:\n",
    "- El modelo ver√° las 1000 im√°genes 10 veces\n",
    "- Total de actualizaciones depende del batch size\n",
    "\n",
    "**Valores t√≠picos:** `10` a `200` (depende del problema)\n",
    "\n",
    "**Efectos:**\n",
    "- **Pocas epochs** ‚Üí Underfitting (modelo no aprende suficiente)\n",
    "- **Muchas epochs** ‚Üí Overfitting (memoriza los datos)\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Batch Size (Tama√±o de Lote)**\n",
    "\n",
    "**¬øQu√© es?**  \n",
    "N√∫mero de ejemplos que el modelo procesa **antes de actualizar** los pesos.\n",
    "\n",
    "**Tipos:**\n",
    "- **Batch Gradient Descent**: Batch size = TODO el dataset\n",
    "- **Mini-batch**: Batch size = 32, 64, 128, 256 (com√∫n)\n",
    "- **Stochastic**: Batch size = 1\n",
    "\n",
    "**Ejemplo con 1000 datos:**\n",
    "```\n",
    "Batch size = 100 ‚Üí 10 actualizaciones por epoch\n",
    "Batch size = 50  ‚Üí 20 actualizaciones por epoch\n",
    "Batch size = 1   ‚Üí 1000 actualizaciones por epoch\n",
    "```\n",
    "\n",
    "**Efectos:**\n",
    "- **Grande (512+)**: Entrenamiento estable pero lento, necesita m√°s RAM\n",
    "- **Peque√±o (16-32)**: M√°s ruidoso pero puede escapar de m√≠nimos locales\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Regularizaci√≥n (Œ±, Œª, C)**\n",
    "\n",
    "**¬øQu√© es?**  \n",
    "T√©cnica para **penalizar modelos complejos** y evitar overfitting.\n",
    "\n",
    "**Tipos principales:**\n",
    "\n",
    "**L1 (Lasso):**\n",
    "```python\n",
    "Loss = Error + Œ± √ó Œ£|pesos|\n",
    "# Efecto: Lleva algunos pesos a exactamente 0 (selecci√≥n autom√°tica de features)\n",
    "```\n",
    "\n",
    "**L2 (Ridge):**\n",
    "```python\n",
    "Loss = Error + Œ± √ó Œ£(pesos¬≤)\n",
    "# Efecto: Reduce todos los pesos hacia 0 (pero no los elimina)\n",
    "```\n",
    "\n",
    "**Par√°metro C en SVM y Logistic Regression:**\n",
    "- `C` = Inverso de la regularizaci√≥n\n",
    "- **C grande** (ej. 100): Poca regularizaci√≥n ‚Üí Puede hacer overfitting\n",
    "- **C peque√±o** (ej. 0.01): Mucha regularizaci√≥n ‚Üí Puede hacer underfitting\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ **Otros Hiperpar√°metros Importantes**\n",
    "\n",
    "**Random Forest / Gradient Boosting:**\n",
    "- `n_estimators`: N√∫mero de √°rboles (t√≠picamente 100-500)\n",
    "- `max_depth`: Profundidad m√°xima de cada √°rbol (3-20)\n",
    "- `min_samples_split`: M√≠nimo de muestras para dividir un nodo\n",
    "- `max_features`: N√∫mero de features a considerar por split\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "- `n_neighbors` (k): N√∫mero de vecinos a considerar (t√≠picamente 3-15)\n",
    "- `weights`: 'uniform' o 'distance'\n",
    "\n",
    "**Support Vector Machines:**\n",
    "- `kernel`: 'linear', 'rbf', 'poly'\n",
    "- `gamma`: Influencia de un solo ejemplo (bajo=lejos, alto=cerca)\n",
    "- `C`: Par√°metro de regularizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Algoritmos de Optimizaci√≥n\n",
    "\n",
    "### üéØ ¬øQu√© Hace un Optimizador?\n",
    "\n",
    "El optimizador decide **c√≥mo actualizar los pesos** para minimizar el error.\n",
    "\n",
    "**Objetivo:** Encontrar el valle m√°s bajo de la \"monta√±a del error\"\n",
    "\n",
    "---\n",
    "\n",
    "## üèÉ Principales Algoritmos de Optimizaci√≥n\n",
    "\n",
    "### 1Ô∏è‚É£ **SGD (Stochastic Gradient Descent)**\n",
    "\n",
    "**El m√°s b√°sico y fundamental.**\n",
    "\n",
    "**¬øC√≥mo funciona?**\n",
    "```python\n",
    "# Actualizaci√≥n simple:\n",
    "peso = peso - learning_rate √ó gradiente\n",
    "```\n",
    "\n",
    "**Analog√≠a:**  \n",
    "Caminas cuesta abajo, pero el camino es ruidoso (zigzagueas).\n",
    "\n",
    "**Pros:**\n",
    "‚úÖ Simple y f√°cil de entender  \n",
    "‚úÖ Funciona bien para problemas convexos  \n",
    "‚úÖ Bajo uso de memoria  \n",
    "\n",
    "**Contras:**\n",
    "‚ùå Necesitas ajustar manualmente el learning rate  \n",
    "‚ùå Puede quedar atrapado en m√≠nimos locales  \n",
    "‚ùå Convergencia lenta  \n",
    "\n",
    "**C√≥digo:**\n",
    "```python\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "modelo = SGDClassifier(\n",
    "    loss='log_loss',        # Para clasificaci√≥n\n",
    "    learning_rate='optimal', # Ajusta learning rate autom√°ticamente\n",
    "    eta0=0.01,              # Learning rate inicial\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "**Cu√°ndo usarlo:**\n",
    "- Datasets muy grandes (millones de muestras)\n",
    "- Cuando necesitas bajo uso de memoria\n",
    "- Problemas de clasificaci√≥n lineal\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **SGD con Momentum**\n",
    "\n",
    "**SGD mejorado con \"inercia\".**\n",
    "\n",
    "**¬øC√≥mo funciona?**\n",
    "```python\n",
    "# Mantiene un \"promedio m√≥vil\" de gradientes anteriores\n",
    "velocidad = momentum √ó velocidad + learning_rate √ó gradiente\n",
    "peso = peso - velocidad\n",
    "```\n",
    "\n",
    "**Analog√≠a:**  \n",
    "Una bola rodando cuesta abajo que gana impulso ‚Üí Va m√°s r√°pido en la direcci√≥n correcta.\n",
    "\n",
    "**Par√°metros:**\n",
    "- `momentum`: T√≠picamente 0.9 (90% del paso anterior)\n",
    "- `nesterov=True`: Versi√≥n mejorada (mira hacia adelante)\n",
    "\n",
    "**Pros:**\n",
    "‚úÖ Converge m√°s r√°pido que SGD b√°sico  \n",
    "‚úÖ Reduce oscilaciones  \n",
    "‚úÖ Mejor para valles estrechos  \n",
    "\n",
    "**C√≥digo:**\n",
    "```python\n",
    "modelo = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,\n",
    "    momentum=0.9,           # Momentum\n",
    "    nesterov=True,          # Nesterov momentum\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "**El m√°s popular actualmente. Combina lo mejor de varios m√©todos.**\n",
    "\n",
    "**¬øC√≥mo funciona?**\n",
    "```python\n",
    "# Mantiene dos promedios m√≥viles:\n",
    "m = Œ≤‚ÇÅ √ó m + (1-Œ≤‚ÇÅ) √ó gradiente           # Momento de primer orden\n",
    "v = Œ≤‚ÇÇ √ó v + (1-Œ≤‚ÇÇ) √ó (gradiente¬≤)        # Momento de segundo orden\n",
    "\n",
    "# Actualizaci√≥n adaptativa:\n",
    "peso = peso - learning_rate √ó m / ‚àö(v + Œµ)\n",
    "```\n",
    "\n",
    "**Analog√≠a:**  \n",
    "Un GPS inteligente que ajusta autom√°ticamente la velocidad seg√∫n el terreno.\n",
    "\n",
    "**Par√°metros:**\n",
    "- `learning_rate`: T√≠picamente 0.001 (default)\n",
    "- `Œ≤‚ÇÅ` (beta1): 0.9 (default) - Decaimiento para el promedio de gradientes\n",
    "- `Œ≤‚ÇÇ` (beta2): 0.999 (default) - Decaimiento para el promedio de gradientes¬≤\n",
    "- `Œµ` (epsilon): 1e-8 - Estabilidad num√©rica\n",
    "\n",
    "**Pros:**\n",
    "‚úÖ Learning rate adaptativo por par√°metro  \n",
    "‚úÖ Funciona bien \"out of the box\"  \n",
    "‚úÖ Converge r√°pido y estable  \n",
    "‚úÖ No necesitas ajustar tanto el learning rate  \n",
    "\n",
    "**Contras:**\n",
    "‚ùå Usa m√°s memoria (guarda promedios m√≥viles)  \n",
    "‚ùå A veces generaliza peor que SGD con momentum  \n",
    "\n",
    "**Nota:** En scikit-learn, Adam est√° disponible principalmente en redes neuronales (MLPClassifier).\n",
    "\n",
    "**C√≥digo (con MLPClassifier):**\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "modelo = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    solver='adam',           # Optimizador Adam\n",
    "    learning_rate_init=0.001,\n",
    "    beta_1=0.9,              # Par√°metro Œ≤‚ÇÅ\n",
    "    beta_2=0.999,            # Par√°metro Œ≤‚ÇÇ\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "**Cu√°ndo usarlo:**\n",
    "- Redes neuronales (es el default)\n",
    "- Cuando no quieres afinar mucho el learning rate\n",
    "- Problemas con datos ruidosos\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **RMSprop (Root Mean Square Propagation)**\n",
    "\n",
    "**Predecesor de Adam, ajusta el learning rate por par√°metro.**\n",
    "\n",
    "**¬øC√≥mo funciona?**\n",
    "```python\n",
    "# Mantiene un promedio m√≥vil de gradientes al cuadrado:\n",
    "v = Œ≤ √ó v + (1-Œ≤) √ó (gradiente¬≤)\n",
    "peso = peso - learning_rate / ‚àö(v + Œµ) √ó gradiente\n",
    "```\n",
    "\n",
    "**Analog√≠a:**  \n",
    "Ajusta tu velocidad seg√∫n qu√© tan empinado est√° el camino localmente.\n",
    "\n",
    "**Par√°metros:**\n",
    "- `learning_rate`: T√≠picamente 0.001\n",
    "- `Œ≤` (rho): 0.9 - Factor de decaimiento\n",
    "\n",
    "**Pros:**\n",
    "‚úÖ Funciona bien para problemas no estacionarios  \n",
    "‚úÖ Mejor que SGD b√°sico para redes neuronales  \n",
    "\n",
    "**Contras:**\n",
    "‚ùå Adam suele funcionar mejor  \n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ **AdaGrad (Adaptive Gradient)**\n",
    "\n",
    "**Ajusta el learning rate para cada par√°metro basado en su historial.**\n",
    "\n",
    "**¬øC√≥mo funciona?**\n",
    "```python\n",
    "# Acumula gradientes al cuadrado (sin decaimiento):\n",
    "v = v + gradiente¬≤\n",
    "peso = peso - learning_rate / ‚àö(v + Œµ) √ó gradiente\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "‚úÖ Excelente para datos dispersos (sparse)  \n",
    "‚úÖ No necesitas ajustar el learning rate manualmente  \n",
    "\n",
    "**Contras:**\n",
    "‚ùå Learning rate decae demasiado r√°pido  \n",
    "‚ùå Puede dejar de aprender prematuramente  \n",
    "\n",
    "---\n",
    "\n",
    "## üìä Comparaci√≥n de Optimizadores\n",
    "\n",
    "| Optimizador | Velocidad | Estabilidad | Ajuste Manual | Uso de Memoria | Mejor Para |\n",
    "|-------------|-----------|-------------|---------------|----------------|------------|\n",
    "| **SGD** | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Problemas convexos |\n",
    "| **SGD + Momentum** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Datasets grandes |\n",
    "| **Adam** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Redes neuronales |\n",
    "| **RMSprop** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Redes recurrentes |\n",
    "| **AdaGrad** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Datos dispersos |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Recomendaciones Pr√°cticas\n",
    "\n",
    "**Para empezar:**\n",
    "1. Usa **Adam** con learning_rate=0.001 (funciona bien en la mayor√≠a de casos)\n",
    "2. Si no converge, prueba **SGD con momentum** y ajusta el learning rate\n",
    "\n",
    "**Para producci√≥n:**\n",
    "1. Compara SGD, SGD+Momentum y Adam\n",
    "2. Usa GridSearch para encontrar el mejor learning rate\n",
    "3. Monitorea las curvas de aprendizaje\n",
    "\n",
    "**Troubleshooting:**\n",
    "- Loss no baja ‚Üí Learning rate muy bajo o muy alto\n",
    "- Loss oscila ‚Üí Learning rate muy alto, prueba momentum\n",
    "- Converge muy lento ‚Üí Learning rate muy bajo o usa Adam\n",
    "- Overfitting ‚Üí Aumenta regularizaci√≥n, reduce epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Demostraci√≥n Pr√°ctica: Efecto del Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Probar diferentes learning rates\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "results_lr = []\n",
    "\n",
    "print(\"üîç Efecto del Learning Rate en SGD:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    modelo = SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        learning_rate='constant',\n",
    "        eta0=lr,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    modelo.fit(X_train_scaled, y_train)\n",
    "    score = modelo.score(X_test_scaled, y_test)\n",
    "    \n",
    "    results_lr.append({'Learning Rate': lr, 'Accuracy': score})\n",
    "    print(f\"Learning Rate = {lr:6.4f}  ‚Üí  Accuracy = {score:.4f}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "df_lr = pd.DataFrame(results_lr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_lr['Learning Rate'], df_lr['Accuracy'], 'o-', linewidth=2, markersize=8)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate (escala log)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Efecto del Learning Rate en el Rendimiento', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Mejor learning rate: {df_lr.loc[df_lr['Accuracy'].idxmax(), 'Learning Rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Comparaci√≥n de Optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Comparar optimizadores en redes neuronales\n",
    "optimizers = {\n",
    "    'SGD b√°sico': MLPClassifier(solver='sgd', learning_rate_init=0.01, momentum=0, \n",
    "                                max_iter=200, random_state=42),\n",
    "    'SGD + Momentum': MLPClassifier(solver='sgd', learning_rate_init=0.01, momentum=0.9,\n",
    "                                   nesterov=True, max_iter=200, random_state=42),\n",
    "    'Adam': MLPClassifier(solver='adam', learning_rate_init=0.001,\n",
    "                         max_iter=200, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"üèÉ Comparaci√≥n de Optimizadores:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_opt = []\n",
    "for name, model in optimizers.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    score = model.score(X_test_scaled, y_test)\n",
    "    n_iterations = model.n_iter_\n",
    "    \n",
    "    results_opt.append({\n",
    "        'Optimizador': name,\n",
    "        'Accuracy': score,\n",
    "        'Iteraciones': n_iterations\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:20s} | Accuracy: {score:.4f} | Iteraciones: {n_iterations}\")\n",
    "\n",
    "df_opt = pd.DataFrame(results_opt)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(df_opt['Optimizador'], df_opt['Accuracy'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy por Optimizador', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_opt['Accuracy']):\n",
    "    axes[0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "axes[1].bar(df_opt['Optimizador'], df_opt['Iteraciones'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_ylabel('Iteraciones hasta convergencia', fontsize=12)\n",
    "axes[1].set_title('Velocidad de Convergencia', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "print(\"   - Adam suele converger m√°s r√°pido\")\n",
    "print(\"   - SGD + Momentum mejora sobre SGD b√°sico\")\n",
    "print(\"   - La elecci√≥n depende del problema espec√≠fico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Feature Engineering - Ingenier√≠a de Caracter√≠sticas\n",
    "\n",
    "### üìñ ¬øQu√© es Feature Engineering?\n",
    "\n",
    "**Feature Engineering** = Arte de **crear nuevas caracter√≠sticas** (features) a partir de los datos existentes para mejorar el modelo.\n",
    "\n",
    "**Analog√≠a:**  \n",
    "Imagina que tienes ingredientes (datos crudos). Feature Engineering es como:\n",
    "- Picar verduras ‚Üí Transformar datos\n",
    "- Mezclar ingredientes ‚Üí Combinar features\n",
    "- Sazonar ‚Üí Escalar/normalizar\n",
    "\n",
    "### üéØ ¬øPor qu√© es importante?\n",
    "\n",
    "> \"Los features importan m√°s que el algoritmo\" - Andrew Ng\n",
    "\n",
    "Un modelo simple con buenos features > Un modelo complejo con features malos\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è T√©cnicas de Feature Engineering\n",
    "\n",
    "### 1Ô∏è‚É£ **Creaci√≥n de Features**\n",
    "\n",
    "#### A) Features de Dominio\n",
    "\n",
    "Crear features usando **conocimiento del problema**.\n",
    "\n",
    "**Ejemplos:**\n",
    "\n",
    "```python\n",
    "# De fecha ‚Üí m√∫ltiples features\n",
    "df['a√±o'] = df['fecha'].dt.year\n",
    "df['mes'] = df['fecha'].dt.month\n",
    "df['d√≠a_semana'] = df['fecha'].dt.dayofweek\n",
    "df['es_fin_semana'] = df['d√≠a_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# De texto ‚Üí longitud\n",
    "df['largo_nombre'] = df['nombre'].str.len()\n",
    "df['num_palabras'] = df['descripcion'].str.split().str.len()\n",
    "\n",
    "# Matem√°ticas del dominio\n",
    "df['IMC'] = df['peso'] / (df['altura'] ** 2)  # √çndice de Masa Corporal\n",
    "df['velocidad'] = df['distancia'] / df['tiempo']\n",
    "```\n",
    "\n",
    "#### B) Features de Interacci√≥n\n",
    "\n",
    "Combinar dos o m√°s features existentes.\n",
    "\n",
    "```python\n",
    "# Multiplicaci√≥n\n",
    "df['area'] = df['largo'] * df['ancho']\n",
    "\n",
    "# Divisi√≥n\n",
    "df['ratio_precio_m2'] = df['precio'] / df['area']\n",
    "\n",
    "# Suma/Resta\n",
    "df['ingresos_netos'] = df['ingresos'] - df['gastos']\n",
    "```\n",
    "\n",
    "#### C) Features Polinomiales\n",
    "\n",
    "Crear potencias e interacciones autom√°ticamente.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Grado 2: x, y ‚Üí x, y, x¬≤, xy, y¬≤\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "```\n",
    "\n",
    "**Ejemplo visual:**\n",
    "```\n",
    "Original: [altura, peso]\n",
    "         ‚Üì\n",
    "Grado 2: [altura, peso, altura¬≤, altura√ópeso, peso¬≤]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Transformaciones**\n",
    "\n",
    "#### A) Transformaci√≥n Logar√≠tmica\n",
    "\n",
    "√ötil para **datos con distribuci√≥n asim√©trica** (skewed).\n",
    "\n",
    "```python\n",
    "df['log_ingreso'] = np.log1p(df['ingreso'])  # log(1 + x) para evitar log(0)\n",
    "```\n",
    "\n",
    "**Antes vs Despu√©s:**\n",
    "```\n",
    "Ingresos originales: [100, 1000, 10000, 100000]\n",
    "Log transformado:    [4.6, 6.9,  9.2,   11.5]    ‚Üê M√°s \"normal\"\n",
    "```\n",
    "\n",
    "#### B) Ra√≠z Cuadrada / Box-Cox\n",
    "\n",
    "```python\n",
    "df['sqrt_area'] = np.sqrt(df['area'])\n",
    "\n",
    "# Box-Cox (encuentra la mejor transformaci√≥n)\n",
    "from scipy.stats import boxcox\n",
    "df['transformed'], lambda_param = boxcox(df['feature'] + 1)\n",
    "```\n",
    "\n",
    "#### C) Binning (Discretizaci√≥n)\n",
    "\n",
    "Convertir variables continuas en categor√≠as.\n",
    "\n",
    "```python\n",
    "# Edad ‚Üí Grupos de edad\n",
    "df['grupo_edad'] = pd.cut(df['edad'], \n",
    "                          bins=[0, 18, 35, 60, 100],\n",
    "                          labels=['Menor', 'Adulto Joven', 'Adulto', 'Adulto Mayor'])\n",
    "\n",
    "# Cuantiles (igual n√∫mero de muestras por bin)\n",
    "df['ingreso_cuantil'] = pd.qcut(df['ingreso'], q=4, \n",
    "                                labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Encoding de Variables Categ√≥ricas**\n",
    "\n",
    "Los modelos necesitan n√∫meros, no texto.\n",
    "\n",
    "#### A) Label Encoding\n",
    "\n",
    "Asignar un n√∫mero a cada categor√≠a.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['ciudad_encoded'] = le.fit_transform(df['ciudad'])\n",
    "\n",
    "# 'Bogot√°' ‚Üí 0, 'Medell√≠n' ‚Üí 1, 'Cali' ‚Üí 2\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Cuidado:** Implica orden (0 < 1 < 2). Solo para variables ordinales.\n",
    "\n",
    "#### B) One-Hot Encoding\n",
    "\n",
    "Crear una columna binaria por cada categor√≠a.\n",
    "\n",
    "```python\n",
    "# Pandas\n",
    "df_encoded = pd.get_dummies(df, columns=['ciudad'])\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')  # drop='first' evita multicolinealidad\n",
    "encoded = ohe.fit_transform(df[['ciudad']])\n",
    "```\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "ciudad       ‚Üí  ciudad_Bogot√°  ciudad_Medell√≠n  ciudad_Cali\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Bogot√°       ‚Üí       1              0              0\n",
    "Medell√≠n     ‚Üí       0              1              0\n",
    "Cali         ‚Üí       0              0              1\n",
    "```\n",
    "\n",
    "#### C) Target Encoding\n",
    "\n",
    "Reemplazar categor√≠a por la media del target.\n",
    "\n",
    "```python\n",
    "# Media del precio por ciudad\n",
    "city_means = df.groupby('ciudad')['precio'].mean()\n",
    "df['ciudad_encoded'] = df['ciudad'].map(city_means)\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Riesgo de data leakage:** Solo usar en train set.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Escalado y Normalizaci√≥n**\n",
    "\n",
    "Poner todas las features en la misma escala.\n",
    "\n",
    "#### A) StandardScaler (Z-score)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# F√≥rmula: (x - media) / desviaci√≥n_est√°ndar\n",
    "# Resultado: Media=0, Std=1\n",
    "```\n",
    "\n",
    "**Antes:**\n",
    "```\n",
    "edad:    [20, 30, 40, 50, 60]\n",
    "salario: [30000, 50000, 70000, 90000, 110000]\n",
    "```\n",
    "\n",
    "**Despu√©s:**\n",
    "```\n",
    "edad:    [-1.41, -0.71, 0, 0.71, 1.41]\n",
    "salario: [-1.41, -0.71, 0, 0.71, 1.41]\n",
    "```\n",
    "\n",
    "#### B) MinMaxScaler\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# F√≥rmula: (x - min) / (max - min)\n",
    "# Resultado: Rango [0, 1]\n",
    "```\n",
    "\n",
    "#### C) RobustScaler\n",
    "\n",
    "Robusto a outliers (usa mediana y cuartiles).\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "**¬øCu√°l usar?**\n",
    "- **StandardScaler**: Default, funciona bien en general\n",
    "- **MinMaxScaler**: Para redes neuronales, cuando quieres [0,1]\n",
    "- **RobustScaler**: Cuando tienes muchos outliers\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ **Feature Selection - Selecci√≥n de Features**\n",
    "\n",
    "**¬øPor qu√©?** M√°s features ‚â† Mejor modelo\n",
    "- Reduce overfitting\n",
    "- Acelera entrenamiento\n",
    "- Mejora interpretabilidad\n",
    "\n",
    "#### A) Filtros (Filter Methods)\n",
    "\n",
    "Seleccionar basado en estad√≠sticas.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Seleccionar las k mejores features\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Ver qu√© features se seleccionaron\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Features seleccionadas:\", selected_features.tolist())\n",
    "```\n",
    "\n",
    "#### B) Wrapper Methods (RFE)\n",
    "\n",
    "Entrenar modelo iterativamente eliminando features.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector = RFE(estimator, n_features_to_select=10, step=1)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "```\n",
    "\n",
    "#### C) Embedded Methods (Feature Importance)\n",
    "\n",
    "Usar importancia del modelo.\n",
    "\n",
    "```python\n",
    "# Random Forest da importancia autom√°ticamente\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Ver importancia\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importances)\n",
    "\n",
    "# Seleccionar top 10\n",
    "top_features = importances.head(10)['feature'].tolist()\n",
    "X_selected = X[top_features]\n",
    "```\n",
    "\n",
    "#### D) Reducci√≥n de Dimensionalidad (PCA)\n",
    "\n",
    "Crear nuevas features que son combinaciones lineales.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)  # Reducir a 10 componentes\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"Varianza explicada: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ **Manejo de Datos Faltantes**\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Estrategias:\n",
    "imputer_mean = SimpleImputer(strategy='mean')      # Media\n",
    "imputer_median = SimpleImputer(strategy='median')  # Mediana (mejor con outliers)\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')  # Moda (categ√≥ricas)\n",
    "imputer_constant = SimpleImputer(strategy='constant', fill_value=0)  # Valor fijo\n",
    "\n",
    "X_imputed = imputer_mean.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Pipeline Completo de Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo completo con dataset real\n",
    "print(\"üèóÔ∏è DEMO: Feature Engineering Completo\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Cargar datos\n",
    "diabetes = load_diabetes(as_frame=True)\n",
    "X_orig = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "print(f\"1Ô∏è‚É£ Dataset original: {X_orig.shape}\")\n",
    "print(f\"   Features: {X_orig.columns.tolist()}\\n\")\n",
    "\n",
    "# 2. Features polinomiales (grado 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_orig)\n",
    "print(f\"2Ô∏è‚É£ Despu√©s de features polinomiales: {X_poly.shape}\")\n",
    "print(f\"   {X_orig.shape[1]} ‚Üí {X_poly.shape[1]} features\\n\")\n",
    "\n",
    "# 3. Selecci√≥n de features (top 20)\n",
    "selector = SelectKBest(score_func=f_classif, k=20)\n",
    "X_selected = selector.fit_transform(X_poly, y)\n",
    "print(f\"3Ô∏è‚É£ Despu√©s de selecci√≥n: {X_selected.shape}\")\n",
    "print(f\"   {X_poly.shape[1]} ‚Üí {X_selected.shape[1]} features\\n\")\n",
    "\n",
    "# 4. Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Comparar: Con vs Sin Feature Engineering\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Sin feature engineering\n",
    "X_train_orig, X_test_orig, _, _ = train_test_split(\n",
    "    X_orig, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "model_baseline = Ridge(alpha=1.0, random_state=42)\n",
    "model_baseline.fit(X_train_orig, y_train)\n",
    "score_baseline = model_baseline.score(X_test_orig, y_test)\n",
    "\n",
    "# Con feature engineering\n",
    "model_engineered = Ridge(alpha=1.0, random_state=42)\n",
    "model_engineered.fit(X_train, y_train)\n",
    "score_engineered = model_engineered.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nüìä RESULTADOS:\\n\")\n",
    "print(f\"   Sin Feature Engineering:  R¬≤ = {score_baseline:.4f}\")\n",
    "print(f\"   Con Feature Engineering:  R¬≤ = {score_engineered:.4f}\")\n",
    "print(f\"   Mejora:                   {(score_engineered - score_baseline):.4f} ({(score_engineered/score_baseline - 1)*100:+.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature Engineering puede mejorar significativamente el modelo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Resumen de Feature Engineering\n",
    "\n",
    "**‚úÖ DO's (Hacer):**\n",
    "1. Entender el dominio del problema\n",
    "2. Visualizar distribuciones antes de transformar\n",
    "3. Crear features de interacci√≥n relevantes\n",
    "4. Aplicar escalado cuando sea necesario\n",
    "5. Eliminar features irrelevantes\n",
    "\n",
    "**‚ùå DON'Ts (No hacer):**\n",
    "1. Aplicar transformaciones sin raz√≥n\n",
    "2. Crear demasiadas features (maldici√≥n de dimensionalidad)\n",
    "3. Usar informaci√≥n del test set\n",
    "4. Ignorar data leakage\n",
    "5. No documentar las transformaciones\n",
    "\n",
    "**üí° Tips:**\n",
    "- Empieza simple, agrega complejidad gradualmente\n",
    "- Usa Pipelines para evitar data leakage\n",
    "- Documenta cada transformaci√≥n\n",
    "- Valida que las features nuevas mejoran el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Modelos de Clasificaci√≥n - Implementaci√≥n Completa\n",
    "\n",
    "Ahora aplicaremos todo lo aprendido sobre hiperpar√°metros y feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y preparar datos\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Divisi√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä Dataset Iris cargado\")\n",
    "print(f\"   Train: {X_train.shape} | Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de modelos con diferentes configuraciones\n",
    "models = {\n",
    "    'Logistic Regression (C=1)': LogisticRegression(C=1.0, max_iter=500, random_state=42),\n",
    "    'Logistic Regression (C=0.1)': LogisticRegression(C=0.1, max_iter=500, random_state=42),\n",
    "    'SGD (lr=0.01)': SGDClassifier(loss='log_loss', learning_rate='constant', \n",
    "                                   eta0=0.01, max_iter=1000, random_state=42),\n",
    "    'SGD (lr=0.1)': SGDClassifier(loss='log_loss', learning_rate='constant',\n",
    "                                  eta0=0.1, max_iter=1000, random_state=42),\n",
    "    'Random Forest (depth=5)': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Random Forest (depth=None)': RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42),\n",
    "    'SVM (C=1)': SVC(C=1.0, kernel='rbf', random_state=42),\n",
    "    'SVM (C=10)': SVC(C=10.0, kernel='rbf', random_state=42),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"üöÄ Entrenando modelos con diferentes hiperpar√°metros...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Entrenar\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': score,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:35s} | Test: {score:.4f} | CV: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RANKING DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Ejercicio Pr√°ctico: Ajustando Hiperpar√°metros\n",
    "\n",
    "**Tu turno:** Experimenta cambiando estos hiperpar√°metros y observa el efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO: Modifica estos valores y observa los cambios\n",
    "\n",
    "# 1. Cambia el learning rate de SGD\n",
    "learning_rate_to_test = 0.01  # Prueba: 0.001, 0.01, 0.1, 1.0\n",
    "\n",
    "modelo_sgd = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    learning_rate='constant',\n",
    "    eta0=learning_rate_to_test,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Cambia el par√°metro C de Logistic Regression\n",
    "C_to_test = 1.0  # Prueba: 0.001, 0.01, 0.1, 1, 10, 100\n",
    "\n",
    "modelo_lr = LogisticRegression(\n",
    "    C=C_to_test,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Cambia max_depth de Random Forest\n",
    "max_depth_to_test = 10  # Prueba: 3, 5, 10, 20, None\n",
    "\n",
    "modelo_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=max_depth_to_test,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "for name, modelo in [('SGD', modelo_sgd), ('LogReg', modelo_lr), ('RF', modelo_rf)]:\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('model', modelo)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print(f\"{name:10s} | Accuracy: {score:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Cambia los valores arriba y vuelve a ejecutar para experimentar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Proyecto Final Integrador\n",
    "\n",
    "**Desaf√≠o:** Crear un sistema completo aplicando todo lo aprendido.\n",
    "\n",
    "### Requisitos:\n",
    "1. ‚úÖ Aplicar Feature Engineering\n",
    "2. ‚úÖ Comparar al menos 5 modelos\n",
    "3. ‚úÖ Probar diferentes hiperpar√°metros\n",
    "4. ‚úÖ Usar GridSearchCV\n",
    "5. ‚úÖ Evaluar con m√∫ltiples m√©tricas\n",
    "6. ‚úÖ Guardar el mejor modelo\n",
    "7. ‚úÖ Documentar decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROYECTO FINAL - TU C√ìDIGO AQU√ç\n",
    "\n",
    "# Paso 1: Cargar datos\n",
    "breast_cancer = load_breast_cancer(as_frame=True)\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "print(\"üéØ PROYECTO: Clasificaci√≥n de C√°ncer de Mama\")\n",
    "print(f\"   Datos: {X.shape}\")\n",
    "print(f\"   Clases: {np.unique(y, return_counts=True)}\")\n",
    "\n",
    "# Paso 2: Feature Engineering\n",
    "# TODO: Aplica transformaciones, selecci√≥n de features, etc.\n",
    "\n",
    "# Paso 3: Divisi√≥n de datos\n",
    "# TODO: train_test_split con stratify\n",
    "\n",
    "# Paso 4: Comparar modelos\n",
    "# TODO: Probar m√∫ltiples modelos con diferentes hiperpar√°metros\n",
    "\n",
    "# Paso 5: Optimizaci√≥n\n",
    "# TODO: GridSearchCV en el mejor modelo\n",
    "\n",
    "# Paso 6: Evaluaci√≥n final\n",
    "# TODO: Matriz de confusi√≥n, classification report, ROC curve\n",
    "\n",
    "# Paso 7: Guardar modelo\n",
    "# TODO: joblib.dump() con metadata\n",
    "\n",
    "print(\"\\n‚úÖ Completa cada TODO y documenta tus decisiones!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Referencias y Recursos\n",
    "\n",
    "### üìñ Hiperpar√°metros\n",
    "- [Scikit-learn: Hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [Practical recommendations for gradient-based training](https://arxiv.org/abs/1206.5533)\n",
    "\n",
    "### üèÉ Optimizaci√≥n\n",
    "- [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)\n",
    "- [Adam paper](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "### üõ†Ô∏è Feature Engineering\n",
    "- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)\n",
    "- [Scikit-learn: Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ¬°Felicitaciones!\n",
    "\n",
    "Has completado el curso completo de Fundamentos de IA. Ahora dominas:\n",
    "\n",
    "‚úÖ **Hiperpar√°metros**: Learning rate, epochs, regularizaci√≥n  \n",
    "‚úÖ **Optimizadores**: SGD, Momentum, Adam, RMSprop  \n",
    "‚úÖ **Feature Engineering**: Creaci√≥n, transformaci√≥n, selecci√≥n  \n",
    "‚úÖ **Modelado**: M√∫ltiples algoritmos y comparaci√≥n  \n",
    "‚úÖ **Optimizaci√≥n**: GridSearch y RandomizedSearch  \n",
    "‚úÖ **Despliegue**: Guardado y versionado de modelos  \n",
    "\n",
    "**Pr√≥ximos pasos:**\n",
    "1. Practica con datasets reales de Kaggle\n",
    "2. Profundiza en Deep Learning\n",
    "3. Aprende MLOps para producci√≥n\n",
    "4. Contribuye a proyectos open source\n",
    "\n",
    "**¬°Sigue aprendiendo! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook creado por Alexander*  \n",
    "*Curso: Fundamentos de IA - Entrenamiento de Modelos*  \n",
    "*√öltima actualizaci√≥n: Noviembre 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
